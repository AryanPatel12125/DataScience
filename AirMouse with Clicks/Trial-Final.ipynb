{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the screen width and height\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define the hand detection frame region (modify these values based on your preference)\n",
    "frame_region_start = (100, 100)\n",
    "frame_region_end = (300, 300)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally (mirror effect)\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Draw the rectangle for the hand detection region\n",
    "    cv2.rectangle(frame, frame_region_start, frame_region_end, (255, 0, 0), 2)\n",
    "\n",
    "    # Extract the region of interest\n",
    "    roi = frame[frame_region_start[1]:frame_region_end[1], frame_region_start[0]:frame_region_end[0]]\n",
    "\n",
    "    # Convert ROI to HSV color space (to detect skin color more effectively)\n",
    "    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define skin color range in HSV\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    # Detect skin color\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        # Find the largest contour and its center\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        M = cv2.moments(max_contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "            # Map the center of the hand to screen coordinates\n",
    "            screen_x = np.interp(cx, (0, frame_region_end[0] - frame_region_start[0]), (0, screen_width))\n",
    "            screen_y = np.interp(cy, (0, frame_region_end[1] - frame_region_start[1]), (0, screen_height))\n",
    "\n",
    "            # Move the cursor\n",
    "            pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Break the loop with the ESC key\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Initializing MediaPipe hand tracking module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get the screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally and convert the color from BGR to RGB\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Check if any hand is detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get the tip of the index finger\n",
    "            tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "            tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "            # Convert the coordinates to screen resolution\n",
    "            screen_x = np.interp(tip_x, [0, 1], [0, screen_width])\n",
    "            screen_y = np.interp(tip_y, [0, 1], [0, screen_height])\n",
    "\n",
    "            # Move the cursor\n",
    "            pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to break\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Initializing MediaPipe hand tracking module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get the screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define the detection frame region (modify these values as needed)\n",
    "# Positioned a bit above the center; adjust based on your preference\n",
    "frame_region = {\n",
    "    \"start_point\": (100, 100),  # Top left corner of the rectangle\n",
    "    \"end_point\": (540, 380),    # Bottom right corner of the rectangle\n",
    "    \"color\": (255, 0, 0),       # Rectangle color (BGR)\n",
    "    \"thickness\": 2,             # Thickness of the rectangle borders\n",
    "}\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally and convert the color from BGR to RGB\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw the detection frame region on the webcam feed\n",
    "    cv2.rectangle(frame, frame_region[\"start_point\"], frame_region[\"end_point\"], frame_region[\"color\"], frame_region[\"thickness\"])\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Check if any hand is detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the tip of the index finger\n",
    "            tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "            tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "            # Convert hand landmark position to relative frame coordinates\n",
    "            relative_x = int(tip_x * frame.shape[1])\n",
    "            relative_y = int(tip_y * frame.shape[0])\n",
    "\n",
    "            # Check if the index finger tip is within the detection frame region\n",
    "            if (frame_region[\"start_point\"][0] < relative_x < frame_region[\"end_point\"][0]) and (frame_region[\"start_point\"][1] < relative_y < frame_region[\"end_point\"][1]):\n",
    "                # Draw hand landmarks\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Map the hand position to the screen size\n",
    "                screen_x = np.interp(relative_x, [frame_region[\"start_point\"][0], frame_region[\"end_point\"][0]], [0, screen_width])\n",
    "                screen_y = np.interp(relative_y, [frame_region[\"start_point\"][1], frame_region[\"end_point\"][1]], [0, screen_height])\n",
    "\n",
    "                # Move the cursor\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to break\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# hand tracking module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get the screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# margins are defined for making the frame region appear at top corner\n",
    "top_margin = 50\n",
    "right_margin = 50\n",
    "frame_width = 300\n",
    "frame_height = 200\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Get the frame dimensions\n",
    "    frame_height_cam, frame_width_cam, _ = frame.shape\n",
    "\n",
    "    frame_region = {\n",
    "        \"start_point\": (frame_width_cam - frame_width - right_margin, top_margin),\n",
    "        \"end_point\": (frame_width_cam - right_margin, top_margin + frame_height),\n",
    "        \"color\": (255, 0, 0), \n",
    "        \"thickness\": 1, \n",
    "    }\n",
    "\n",
    "    # Flip the frame horizontally and convert the color from BGR to RGB - generated by GPT because it was having some error\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw the detection frame region on the webcam feed\n",
    "    cv2.rectangle(frame, frame_region[\"start_point\"], frame_region[\"end_point\"], frame_region[\"color\"], frame_region[\"thickness\"])\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Check if any hand is detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the tip of the index finger\n",
    "            tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "            tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "            # Convert hand landmark position to relative frame coordinates\n",
    "            relative_x = int(tip_x * frame_width_cam)\n",
    "            relative_y = int(tip_y * frame_height_cam)\n",
    "\n",
    "            # Check if the index finger tip is within the detection frame region\n",
    "            if (frame_region[\"start_point\"][0] < relative_x < frame_region[\"end_point\"][0]) and (frame_region[\"start_point\"][1] < relative_y < frame_region[\"end_point\"][1]):\n",
    "                # Draw hand landmarks\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Map the hand position to the screen size\n",
    "                screen_x = np.interp(relative_x, [frame_region[\"start_point\"][0], frame_region[\"end_point\"][0]], [0, screen_width])\n",
    "                screen_y = np.interp(relative_y, [frame_region[\"start_point\"][1], frame_region[\"end_point\"][1]], [0, screen_height])\n",
    "\n",
    "                # Move the cursor\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to break\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
