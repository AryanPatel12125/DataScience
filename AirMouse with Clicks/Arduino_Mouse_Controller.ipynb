{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# hand tracking module\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get the screen size\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# margins are defined for making the frame region appear at top corner\n",
    "top_margin = 50\n",
    "right_margin = 50\n",
    "frame_width = 300\n",
    "frame_height = 200\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Get the frame dimensions\n",
    "    frame_height_cam, frame_width_cam, _ = frame.shape\n",
    "\n",
    "    frame_region = {\n",
    "        \"start_point\": (frame_width_cam - frame_width - right_margin, top_margin),\n",
    "        \"end_point\": (frame_width_cam - right_margin, top_margin + frame_height),\n",
    "        \"color\": (255, 0, 0), \n",
    "        \"thickness\": 1, \n",
    "    }\n",
    "\n",
    "    # Flip the frame horizontally and convert the color from BGR to RGB - generated by GPT because it was having some error\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Draw the detection frame region on the webcam feed\n",
    "    cv2.rectangle(frame, frame_region[\"start_point\"], frame_region[\"end_point\"], frame_region[\"color\"], frame_region[\"thickness\"])\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Check if any hand is detected\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the tip of the index finger\n",
    "            tip_x = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x\n",
    "            tip_y = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y\n",
    "\n",
    "            # Convert hand landmark position to relative frame coordinates\n",
    "            relative_x = int(tip_x * frame_width_cam)\n",
    "            relative_y = int(tip_y * frame_height_cam)\n",
    "\n",
    "            # Check if the index finger tip is within the detection frame region\n",
    "            if (frame_region[\"start_point\"][0] < relative_x < frame_region[\"end_point\"][0]) and (frame_region[\"start_point\"][1] < relative_y < frame_region[\"end_point\"][1]):\n",
    "                # Draw hand landmarks\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Map the hand position to the screen size\n",
    "                screen_x = np.interp(relative_x, [frame_region[\"start_point\"][0], frame_region[\"end_point\"][0]], [0, screen_width])\n",
    "                screen_y = np.interp(relative_y, [frame_region[\"start_point\"][1], frame_region[\"end_point\"][1]], [0, screen_height])\n",
    "\n",
    "                # Move the cursor\n",
    "                pyautogui.moveTo(screen_x, screen_y)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to break\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
