{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_qNcbYiXDC"
   },
   "source": [
    "# **Dataset Information**\n",
    "\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. \n",
    "\n",
    "High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. \n",
    "\n",
    "Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
    "\n",
    "\n",
    "# **Attribute Information**\n",
    "\n",
    "To construct the data, seven geometric parameters of wheat kernels were measured:\n",
    "1. area A\n",
    "2. perimeter P\n",
    "3. compactness C = 4*(pi)*(A)/(P^2)\n",
    "4. length of kernel\n",
    "5. width of kernel\n",
    "6. asymmetry coefficient\n",
    "7. length of kernel groove\n",
    "\n",
    "All of these parameters were real-valued continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9R1rkTRV5Nrz",
    "outputId": "49fd1263-9f02-471a-ea43-0988b1accc2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in g:\\anaconda\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in g:\\anaconda\\lib\\site-packages (from plotly) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install plotly\n",
    "\n",
    "from IPython.display import HTML\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RMOOhRG75XMY"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/Wheat_dataset - Copy.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/Wheat_dataset - Copy.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy(deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy(deep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    487\u001b[0m     )\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mG:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Wheat_dataset - Copy.xlsx'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Wheat_dataset1.xlsx')\n",
    "df1 = df.copy(deep = True)\n",
    "df2 = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "-EG-pCmp_dxk",
    "outputId": "35ac40b3-b215-445e-a585-2330c8f94f79"
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k8Mx9m5e5XI-",
    "outputId": "25513551-db5e-4be1-9fe8-b916ae1f75ed"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMxAKmrZ8dC8"
   },
   "source": [
    "Clustering algorithms like K-means require feature scaling of the data as part of data preprocessing to produce good results. This is because clustering techniques use distance calculation between the data points. Hence it is proper to bring data of different units under a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "457DoSSY5XD0"
   },
   "outputs": [],
   "source": [
    "# STANDARDIZING\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_scaled = scaler.fit_transform(df1)\n",
    "x_scaled = pd.DataFrame(x_scaled, columns = df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "3YBh2BwaA5tj",
    "outputId": "b74de1f8-bebf-47db-b58c-0734958fc19c"
   },
   "outputs": [],
   "source": [
    "x_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwK8IvtT8Krw"
   },
   "source": [
    "To keep the example simple and to visualize the clustering on a 2-D graph we will use only two attributes. \n",
    "\n",
    "Later we will also see after this how you can use more than 2 attributes for clustering and still visualize the results in 2-D with the help of Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58XrhmaW_ynZ"
   },
   "outputs": [],
   "source": [
    "x_scaled1 = x_scaled[['compactness', 'asymmetry_coef']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45J9GACc_-Ur"
   },
   "outputs": [],
   "source": [
    "x_scaled2 = x_scaled1.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uTLxFqV8nHx"
   },
   "source": [
    "\n",
    "Let us see how to apply K-Means in Sklearn to group the dataset clusters (0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTQ4zmGsAeKQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters = 2)\n",
    "pred = model.fit_predict(x_scaled1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LyuQ2vVcQikw",
    "outputId": "d04f93e4-16c4-4fbc-b43b-eb8a3199e121"
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOJvHC3783eU"
   },
   "source": [
    "Add the clusters to their respective data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YTjVgfIBFMR",
    "outputId": "12210ecb-26d6-45c8-8946-d628eb80c8a3"
   },
   "outputs": [],
   "source": [
    "x_scaled1['Clusters'] = pred\n",
    "print(x_scaled1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zgSSZ9t9Dke"
   },
   "source": [
    "From the graph, it is evident that there is a scope for data to be grouped into more clusters than only 2. But how to know how many clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "oWNr6VmIHdkj",
    "outputId": "91cc03d4-2038-4872-df4e-e7e624efa600"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = \"compactness\", y = \"asymmetry_coef\", hue = 'Clusters',  data = x_scaled1 ,palette = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTIu95ua9WI5"
   },
   "source": [
    "\n",
    "The tricky part with K-Means clustering is you do not know in advance that in how many clusters the given data can be divided (hence it is an unsupervised learning algorithm). It can be done with the trial and error method but let us see a more proper technique for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d_gBAjn9ckA"
   },
   "source": [
    "A.) The Elbow Method is a popular technique for determining the optimal number of clusters. Here, we calculate the Within-Cluster-Sum of Squared Errors (WCSS) for various values of k and choose the k for which WSS first starts to diminish.\n",
    "\n",
    "  1.) The Squared Error for a data point is the square of the distance of a point from its cluster center.\n",
    "\n",
    "  2.) The WSS score is the summation of Squared Errors for all given data points.\n",
    "\n",
    "  3.) Distance metrics like Euclidean Distance or the Manhattan Distance can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgXPtDon9gQG"
   },
   "source": [
    "Continuing with our example, we calculate the WCSS for K=2 to k=12 and calculate the WCSS in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58id0kw2BiEA"
   },
   "outputs": [],
   "source": [
    "K = range(2, 12)\n",
    "wss = []\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans=kmeans.fit(x_scaled2)\n",
    "    wss_iter = kmeans.inertia_\n",
    "    wss.append(wss_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tmn4dQYoCQxP",
    "outputId": "b3c92d72-cbd1-4fd0-896f-d9d423083e92"
   },
   "outputs": [],
   "source": [
    "wss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWnhELGN9wuW"
   },
   "source": [
    "It can be seen below that there is an elbow bend at K=3 or K=4 i.e. it is the point after which WCSS does not diminish much with the increase in value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "__O_RVawCWVi",
    "outputId": "f90afc02-73ee-41c1-bd03-a50816f30478"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Within-Cluster-Sum of Squared Errors (WSS)')\n",
    "plt.xticks(range(2,14))\n",
    "plt.plot(K,wss, '-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfhM2uHFGNfA"
   },
   "source": [
    "To get the elbow point we use the KneeLocator library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Y41caLAEjAP",
    "outputId": "799f02ae-1473-41e4-da09-83d719337ce2"
   },
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "from kneed import KneeLocator\n",
    "\n",
    "kl = KneeLocator(range(2, 12), wss, curve=\"convex\", direction=\"decreasing\")\n",
    "print('the elbow point is : ', kl.elbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GApvKY6dGBI9"
   },
   "source": [
    "B.) The silhouette value measures the similarity of a data point within its cluster. It has a range between +1 and -1 and the higher values denote a good clustering.\n",
    "\n",
    "Below we calculate the Silhouette Score for k=2 to 12 and it can be seen that the maximum value is for k=2 and k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFL3AC0pCj_w",
    "outputId": "f850a9fb-65bd-46fd-bb62-ed66236e211f"
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster as cluster\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "for i in range(2,13):\n",
    "  labels = KMeans(n_clusters=i, random_state=200).fit(x_scaled2).labels_\n",
    "  print (\"Silhouette score for k = \" + str(i) + \" is \" + str(metrics.silhouette_score(x_scaled2, labels, metric=\"euclidean\", random_state=200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "kOclSGcVD3g4",
    "outputId": "68cdd49d-d7a1-4ad2-9f66-cb30c3872697"
   },
   "outputs": [],
   "source": [
    "x_scaled2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "SWypacMZDSBo",
    "outputId": "2e1c8c5c-64f8-40c1-9c13-2e3eaf23060b"
   },
   "outputs": [],
   "source": [
    "model1 = KMeans(n_clusters = 4)\n",
    "model1.fit(x_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jVVuSjbIPdK",
    "outputId": "7424d6c9-0f6d-4adb-9b0c-8e2f8c629140"
   },
   "outputs": [],
   "source": [
    "model1.predict(x_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "TSM6H-6nFJaA",
    "outputId": "a2fc3a60-b9a3-4012-9a6e-83bc4db52c0e"
   },
   "outputs": [],
   "source": [
    "x_scaled2['Clusters'] = model1.labels_\n",
    "x_scaled2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "rdz-5AhXIXDg",
    "outputId": "93f498c9-ad95-474a-d1c7-fd14ab092031"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"compactness\", y=\"asymmetry_coef\", hue = 'Clusters',  data=x_scaled2, palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4377xB4QCP1"
   },
   "source": [
    "# Clustering with more than 2 features\n",
    "In the above example, we used only two attributes to perform clustering because it is easier for us to visualize the results in 2-D graph. \n",
    "\n",
    "We cannot visualize anything beyond 3 attributes in 3-D and in real-world scenarios there can be hundred of attributes. So how can we visualize the clustering results?\n",
    "\n",
    "Well, it can be done by applying principal component analysis (PCA) on the dataset to reduce its dimension to only two while still preserving the information. \n",
    "\n",
    "And then clustering can be applied to this transformed dataset and then visualized in a 2-D plot. Moreover, PCA can also help to avoid the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx3GJY2So0IL"
   },
   "source": [
    "#What is Dimensionality reduction?\n",
    "\n",
    "Dimensionality reduction algorithms project high-dimensional data to a low-dimensional space while keeping as much of the variance in the original dataset as possible\n",
    "\n",
    "#Why do we need Dimensionality Reduction algorithms?\n",
    "A large number of features requires a lot of computer resources, and a longer period of time to train. The calculations between the data points will become complex and harder when the number of dimensions is very high in the data. \n",
    "\n",
    "That kind of problem is often referred to as the curse of dimensionality in the context of machine learning.\n",
    "\n",
    "Once the dimensionality has been reduced, machine learning algorithms will be able to perform calculations very effectively and efficiently during training.\n",
    "\n",
    "#What is PCA?\n",
    "\n",
    "PCA is a linear dimensionality reduction technique. It transforms a set of variables (p) into a smaller k (k<p) number of variables called \"principal components\" while retaining as much of the variation in the original dataset as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ttZxcMZQIjpa",
    "outputId": "ed19494b-9c9a-4830-d9aa-d251cf25353a"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vkDOeKQYRGAD",
    "outputId": "7050cca9-f17a-4c8a-ffe2-1953b1d07025"
   },
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler()\n",
    "df2 = pd.DataFrame(scaler2.fit_transform(df2), columns = df2.columns)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD8rf-oNATIi"
   },
   "source": [
    "First we will use all the features to see which features explain the most variance. Then we will use only those features in the actual PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "c_lhw7dA5XBV",
    "outputId": "39443e31-0d2f-4711-ad63-953a9ac927dd"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XgB37B2_IwLm",
    "outputId": "150342fc-66c3-47ab-a16c-5ae7d21d4ffe"
   },
   "outputs": [],
   "source": [
    "print(pca.n_components_)\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JSUglynAbmw"
   },
   "source": [
    "we see that first 3 features explain almost 99% of the variance present in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MkcRcQhKqi2Q",
    "outputId": "bfdb8181-ee59-4fda-aa3d-c5b873656f9f"
   },
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "X3oxAYMC5W-3",
    "outputId": "965efeb4-856c-4c2c-a947-f17fc98fffd6"
   },
   "outputs": [],
   "source": [
    "# GET THE VARIANCE OF EACH FEATURES\n",
    "import matplotlib.pyplot as plt\n",
    "features = range(0, pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VD5LJycUAi9K"
   },
   "source": [
    "SELECT FEATURES WITH HIGH VARIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfH_J0GX5W8g",
    "outputId": "ef512293-46d4-416b-d7d6-254fff155618"
   },
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components = 3)\n",
    "pca1.fit(x_scaled)\n",
    "pca_features = pd.DataFrame(pca1.transform(x_scaled), columns = ['pca1', 'pca2', 'pca3'])\n",
    "print(pca_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "x03F4VYaJGLa",
    "outputId": "63aeb1d8-dd56-4a1e-ebb1-0f69173a69ac"
   },
   "outputs": [],
   "source": [
    "pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wkfHSe05W3x"
   },
   "outputs": [],
   "source": [
    "########## K MEANS CLUSTERING ############\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cost = []\n",
    "for i in range(2,13):\n",
    "    model = KMeans(n_clusters = i)\n",
    "    model.fit(pca_features)\n",
    "    cost.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92iY63ItD3ba",
    "outputId": "51b0c341-5fd1-40ec-b8d7-aa43ca865e4c"
   },
   "outputs": [],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "L4zlU2Mw5W1R",
    "outputId": "fe28b969-babd-4bf4-f9df-2102ca14c65f"
   },
   "outputs": [],
   "source": [
    "# ELBOW PLOT\n",
    "plt.plot(range(2,13), cost, '-o')\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('cost value')\n",
    "plt.xticks(range(2,13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqQ9-tlFDAc9",
    "outputId": "f1e267ba-1a5f-48f8-a6f7-5180b6aa6f83"
   },
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "kl = KneeLocator(range(2, 13), cost, curve=\"convex\", direction=\"decreasing\")\n",
    "print('the elbow point is : ', kl.elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXPsgWQjDYQd",
    "outputId": "70d8163d-2f79-428e-ed0e-8e2ce5d9dbe2"
   },
   "outputs": [],
   "source": [
    "for i in range(2,13):\n",
    "  labels = KMeans(n_clusters=i, random_state=200).fit(pca_features).labels_\n",
    "  print (\"Silhouette score for k = \" + str(i) + \" is \" + str(metrics.silhouette_score(pca_features, labels, metric=\"euclidean\", random_state=200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "gEqaxUEA5Wwa",
    "outputId": "1d771863-005e-4322-a97b-6a6c5b27c9a8"
   },
   "outputs": [],
   "source": [
    "# TRAIN WITH KNEE VALUE\n",
    "model2 = KMeans(n_clusters = 3)\n",
    "model2.fit(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_81Dk54W8jo1"
   },
   "outputs": [],
   "source": [
    "pred2 = model2.predict(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUP8jB7c9-Fj"
   },
   "outputs": [],
   "source": [
    "pca_features['Clusters'] = pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "345wrRBx-DhN",
    "outputId": "212481f9-3f56-4822-9e13-0f2ad8904a26"
   },
   "outputs": [],
   "source": [
    "pca_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "TiETfXI--Jqr",
    "outputId": "6a6c6b04-8652-4850-e5e6-b8a6a262a6e2"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"pca1\", y=\"pca2\", hue = 'Clusters',  data = pca_features, palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "8Ym3pL4nJ_mt",
    "outputId": "73813937-6d18-4928-c2b3-cc5fab23377b"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"pca1\", y=\"pca3\", hue = 'Clusters',  data = pca_features, palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "ktrKsMipKEQ1",
    "outputId": "62b6fe28-93ee-4b01-9f0f-9b398ed5c8bd"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"pca2\", y=\"pca3\", hue = 'Clusters',  data = pca_features, palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "0WWkBm4uaeNn",
    "outputId": "94a23d37-f900-4763-f2fb-95265d8c9cc7"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "fig = plt.figure(figsize = (12, 8))\n",
    "ax = plt.axes(projection = '3d')\n",
    "\n",
    "sctt = ax.scatter3D(pca_features['pca1'], pca_features['pca2'], pca_features['pca3'], c = pca_features['Clusters'], s = 50, alpha = 0.6)\n",
    "\n",
    "plt.title('3D scatterplot of clusters')\n",
    "ax.set_xlabel('pca1')\n",
    "ax.set_ylabel('pca2')\n",
    "ax.set_zlabel('pca3')\n",
    "plt.savefig('3dplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "BtunFKu0MCVe",
    "outputId": "90f58514-d5c6-47b5-abc3-4e65dfe57ef0"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pca_features, x = 'pca1', y = 'pca2', z = 'pca3', color = 'Clusters')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "CFeQO1CqOT9X",
    "outputId": "420d56ed-640d-43f0-ba26-8ae211073c5e"
   },
   "outputs": [],
   "source": [
    "pca_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0hqrktwRitJ"
   },
   "source": [
    "Testing on untrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gN8BgkwI5WnE"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_excel('test_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "IFcVItwxPvu-",
    "outputId": "78049c5f-2087-4f01-be9a-65b0a4329535"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZGiaIZJ5Wks"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(scaler2.transform(test_df), columns = test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8_ydYsU5Wib"
   },
   "outputs": [],
   "source": [
    "test_pca_df = pd.DataFrame(pca1.transform(test_df), columns = ['pca1', 'pca2', 'pca3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "bTqA3ZyWOzeX",
    "outputId": "750486a1-8e09-47a3-8571-20963b3f742c"
   },
   "outputs": [],
   "source": [
    "test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj9-WByAO8yf"
   },
   "outputs": [],
   "source": [
    "test_pred = model2.predict(test_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHmvKxp7PIbM"
   },
   "outputs": [],
   "source": [
    "test_df['Clusters'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "STXXrGi0SBPs",
    "outputId": "cf01b3df-6015-4a80-fac7-b2871cbf7426"
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MngvXY-cBHjw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
