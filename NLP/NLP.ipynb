{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp('''In the realm of Natural Language Processing (NLP), a diverse array of data types presents both challenges and opportunities. Consider a scenario where the text mentions the price of a product, like \"The price of the new gadget is $599.99, and it includes free shipping.\" In this context, NLP can be employed to extract and analyze the currency value, providing valuable insights into pricing trends. Similarly, when dealing with communication data, such as \"For inquiries, please contact us at support@example.com,\" NLP techniques come into play to identify and extract email addresses. Numeric data, as exemplified by \"In the experiment, the temperature ranged from 25.5 to 30.2 degrees Celsius,\" offers opportunities for NLP practitioners to extract and analyze numerical information. Dates, URLs, phone numbers, percentages, and geographical coordinates in various contexts further showcase the versatility of NLP applications. As data scientists navigate this landscape, they harness algorithms to unveil patterns, trends, and meaningful insights across a spectrum of data types, contributing to the ever-evolving field of NLP.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "the\n",
      "realm\n",
      "of\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      "(\n",
      "NLP\n",
      ")\n",
      ",\n",
      "a\n",
      "diverse\n",
      "array\n",
      "of\n",
      "data\n",
      "types\n",
      "presents\n",
      "both\n",
      "challenges\n",
      "and\n",
      "opportunities\n",
      ".\n",
      "Consider\n",
      "a\n",
      "scenario\n",
      "where\n",
      "the\n",
      "text\n",
      "mentions\n",
      "the\n",
      "price\n",
      "of\n",
      "a\n",
      "product\n",
      ",\n",
      "like\n",
      "\"\n",
      "The\n",
      "price\n",
      "of\n",
      "the\n",
      "new\n",
      "gadget\n",
      "is\n",
      "$\n",
      "599.99\n",
      ",\n",
      "and\n",
      "it\n",
      "includes\n",
      "free\n",
      "shipping\n",
      ".\n",
      "\"\n",
      "In\n",
      "this\n",
      "context\n",
      ",\n",
      "NLP\n",
      "can\n",
      "be\n",
      "employed\n",
      "to\n",
      "extract\n",
      "and\n",
      "analyze\n",
      "the\n",
      "currency\n",
      "value\n",
      ",\n",
      "providing\n",
      "valuable\n",
      "insights\n",
      "into\n",
      "pricing\n",
      "trends\n",
      ".\n",
      "Similarly\n",
      ",\n",
      "when\n",
      "dealing\n",
      "with\n",
      "communication\n",
      "data\n",
      ",\n",
      "such\n",
      "as\n",
      "\"\n",
      "For\n",
      "inquiries\n",
      ",\n",
      "please\n",
      "contact\n",
      "us\n",
      "at\n",
      "support@example.com\n",
      ",\n",
      "\"\n",
      "NLP\n",
      "techniques\n",
      "come\n",
      "into\n",
      "play\n",
      "to\n",
      "identify\n",
      "and\n",
      "extract\n",
      "email\n",
      "addresses\n",
      ".\n",
      "Numeric\n",
      "data\n",
      ",\n",
      "as\n",
      "exemplified\n",
      "by\n",
      "\"\n",
      "In\n",
      "the\n",
      "experiment\n",
      ",\n",
      "the\n",
      "temperature\n",
      "ranged\n",
      "from\n",
      "25.5\n",
      "to\n",
      "30.2\n",
      "degrees\n",
      "Celsius\n",
      ",\n",
      "\"\n",
      "offers\n",
      "opportunities\n",
      "for\n",
      "NLP\n",
      "practitioners\n",
      "to\n",
      "extract\n",
      "and\n",
      "analyze\n",
      "numerical\n",
      "information\n",
      ".\n",
      "Dates\n",
      ",\n",
      "URLs\n",
      ",\n",
      "phone\n",
      "numbers\n",
      ",\n",
      "percentages\n",
      ",\n",
      "and\n",
      "geographical\n",
      "coordinates\n",
      "in\n",
      "various\n",
      "contexts\n",
      "further\n",
      "showcase\n",
      "the\n",
      "versatility\n",
      "of\n",
      "NLP\n",
      "applications\n",
      ".\n",
      "As\n",
      "data\n",
      "scientists\n",
      "navigate\n",
      "this\n",
      "landscape\n",
      ",\n",
      "they\n",
      "harness\n",
      "algorithms\n",
      "to\n",
      "unveil\n",
      "patterns\n",
      ",\n",
      "trends\n",
      ",\n",
      "and\n",
      "meaningful\n",
      "insights\n",
      "across\n",
      "a\n",
      "spectrum\n",
      "of\n",
      "data\n",
      "types\n",
      ",\n",
      "contributing\n",
      "to\n",
      "the\n",
      "ever\n",
      "-\n",
      "evolving\n",
      "field\n",
      "of\n",
      "NLP\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['support@example.com']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''Dayton high school, 8th grade students information\n",
    "==================================================\n",
    "\n",
    "Name\tbirth day   \temail\n",
    "-----\t------------\t------\n",
    "Virat   5 June, 1882    virat@kohli.com\n",
    "Maria\t12 April, 2001  maria@sharapova.com\n",
    "Serena  24 June, 1998   serena@williams.com \n",
    "Joe      1 May, 1997    joe@root.com''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virat@kohli.com\n",
      "maria@sharapova.com\n",
      "serena@williams.com\n",
      "joe@root.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''Ravi and Raju are the best friends from school days.They wanted to go for a world tour and \n",
    "visit famous cities like Paris, London, Dubai, Rome etc and also they called their another friend Mohan to take part of this world tour.\n",
    "They started their journey from Hyderabad and spent next 3 months travelling all the wonderful cities in the world and cherish a happy moments!\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_nouns = []\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        proper_nouns.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Raju, Paris, London, Dubai, Rome, Mohan, Hyderabad]\n"
     ]
    }
   ],
   "source": [
    "print(proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''The Top 5 companies in USA are Tesla Inc, Walmart, Amazon, Microsoft, Google and the top 5 companies in \n",
    "India are Infosys, Reliance, HDFC Bank, Hindustan Unilever and Bharti Airtel''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Error downloading 'averaged_perceptron_tagger' from\n",
      "[nltk_data]    |     <https://raw.githubusercontent.com/nltk/nltk_data\n",
      "[nltk_data]    |     /gh-pages/packages/taggers/averaged_perceptron_ta\n",
      "[nltk_data]    |     gger.zip>:   <urlopen error [WinError 10054] An\n",
      "[nltk_data]    |     existing connection was forcibly closed by the\n",
      "[nltk_data]    |     remote host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = ['running', 'painting', 'walking', 'dressing', 'likely', 'children', 'whom', 'good', 'ate', 'fishing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running | run\n",
      "painting | paint\n",
      "walking | walk\n",
      "dressing | dress\n",
      "likely | like\n",
      "children | children\n",
      "whom | whom\n",
      "good | good\n",
      "ate | ate\n",
      "fishing | fish\n",
      "Ravi | ravi\n",
      "and | and\n",
      "Raju | Raju\n",
      "are | be\n",
      "the | the\n",
      "best | good\n",
      "friends | friend\n",
      "from | from\n",
      "school | school\n",
      "days | day\n",
      ". | .\n",
      "They | they\n",
      "wanted | want\n",
      "to | to\n",
      "go | go\n",
      "for | for\n",
      "a | a\n",
      "world | world\n",
      "tour | tour\n",
      "and | and\n",
      "\n",
      " | \n",
      "\n",
      "visit | visit\n",
      "famous | famous\n",
      "cities | city\n",
      "like | like\n",
      "Paris | Paris\n",
      ", | ,\n",
      "London | London\n",
      ", | ,\n",
      "Dubai | Dubai\n",
      ", | ,\n",
      "Rome | Rome\n",
      "etc | etc\n",
      "and | and\n",
      "also | also\n",
      "they | they\n",
      "called | call\n",
      "their | their\n",
      "another | another\n",
      "friend | friend\n",
      "Mohan | Mohan\n",
      "to | to\n",
      "take | take\n",
      "part | part\n",
      "of | of\n",
      "this | this\n",
      "world | world\n",
      "tour | tour\n",
      ". | .\n",
      "\n",
      " | \n",
      "\n",
      "They | they\n",
      "started | start\n",
      "their | their\n",
      "journey | journey\n",
      "from | from\n",
      "Hyderabad | Hyderabad\n",
      "and | and\n",
      "spent | spend\n",
      "next | next\n",
      "3 | 3\n",
      "months | month\n",
      "travelling | travel\n",
      "all | all\n",
      "the | the\n",
      "wonderful | wonderful\n",
      "cities | city\n",
      "in | in\n",
      "the | the\n",
      "world | world\n",
      "and | and\n",
      "cherish | cherish\n",
      "a | a\n",
      "happy | happy\n",
      "moments | moment\n",
      "! | !\n",
      "\n",
      " | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for word in list_words:\n",
    "    print(f\"{word} | {stemmer.stem(word)}\")\n",
    "    \n",
    "for token in doc:\n",
    "    print(f\"{token} | {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ravi | NOUN |\n",
      "and | CCONJ |\n",
      "Raju | PROPN |\n",
      "are | AUX |\n",
      "the | DET |\n",
      "best | ADJ |\n",
      "friends | NOUN |\n",
      "from | ADP |\n",
      "school | NOUN |\n",
      "days | NOUN |\n",
      ". | PUNCT |\n",
      "They | PRON |\n",
      "wanted | VERB |\n",
      "to | PART |\n",
      "go | VERB |\n",
      "for | ADP |\n",
      "a | DET |\n",
      "world | NOUN |\n",
      "tour | NOUN |\n",
      "and | CCONJ |\n",
      "\n",
      " | SPACE |\n",
      "visit | VERB |\n",
      "famous | ADJ |\n",
      "cities | NOUN |\n",
      "like | ADP |\n",
      "Paris | PROPN |\n",
      ", | PUNCT |\n",
      "London | PROPN |\n",
      ", | PUNCT |\n",
      "Dubai | PROPN |\n",
      ", | PUNCT |\n",
      "Rome | PROPN |\n",
      "etc | X |\n",
      "and | CCONJ |\n",
      "also | ADV |\n",
      "they | PRON |\n",
      "called | VERB |\n",
      "their | PRON |\n",
      "another | DET |\n",
      "friend | NOUN |\n",
      "Mohan | PROPN |\n",
      "to | PART |\n",
      "take | VERB |\n",
      "part | NOUN |\n",
      "of | ADP |\n",
      "this | DET |\n",
      "world | NOUN |\n",
      "tour | NOUN |\n",
      ". | PUNCT |\n",
      "\n",
      " | SPACE |\n",
      "They | PRON |\n",
      "started | VERB |\n",
      "their | PRON |\n",
      "journey | NOUN |\n",
      "from | ADP |\n",
      "Hyderabad | PROPN |\n",
      "and | CCONJ |\n",
      "spent | VERB |\n",
      "next | ADV |\n",
      "3 | NUM |\n",
      "months | NOUN |\n",
      "travelling | VERB |\n",
      "all | DET |\n",
      "the | DET |\n",
      "wonderful | ADJ |\n",
      "cities | NOUN |\n",
      "in | ADP |\n",
      "the | DET |\n",
      "world | NOUN |\n",
      "and | CCONJ |\n",
      "cherish | VERB |\n",
      "a | DET |\n",
      "happy | ADJ |\n",
      "moments | NOUN |\n",
      "! | PUNCT |\n",
      "\n",
      " | SPACE |\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token} | {token.pos_} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spam_count(data):\n",
    "    if data['Category'] == \"spam\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['spam'] = data.apply(get_spam_count, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(data['Message'],data['spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "v = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7522 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56065 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv = v.fit_transform(x_train.values)\n",
    "x_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cv = v.transform(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
